{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "f6eFaBvki17J"
      },
      "outputs": [],
      "source": [
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = 'all'\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "from scipy.linalg import norm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('sample_data/california_housing_train.csv')\n",
        "df1=pd.read_csv('sample_data/california_housing_test.csv')"
      ],
      "metadata": {
        "id": "YTeVRq6ojKA4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = df['median_house_value'].values\n",
        "A=df.drop(['median_house_value'],axis =1).values"
      ],
      "metadata": {
        "id": "77eTDFiijKDV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "A=A[:500]\n",
        "y=y[:500]\n",
        "\n",
        "mA = A.mean(axis=0)\n",
        "sA = A.std(axis=0)\n",
        "A = (A-mA)/sA\n",
        "m = y.mean()\n",
        "y = y-m"
      ],
      "metadata": {
        "id": "dgxMoVzQjKF-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RegPb(object):\n",
        "    '''                                                                   \n",
        "        A class for regression problems with linear models.\n",
        "        \n",
        "        Attributes:\n",
        "            A: Data matrix (features)\n",
        "            y: Data vector (labels)\n",
        "            n,d: Dimensions of A\n",
        "            loss: Loss function to be considered in the regression\n",
        "                'l2': Least-squares loss\n",
        "                'logit': Logistic loss\n",
        "            lbda: Regularization parameter\n",
        "    '''\n",
        "   \n",
        "    # Instantiate the class\n",
        "    def __init__(self, A, y,lbda=0):\n",
        "        self.A = A\n",
        "        self.y = y\n",
        "        self.n, self.p = A.shape\n",
        "       \n",
        "        self.lbda = lbda\n",
        "        \n",
        "    \n",
        "    # Objective value\n",
        "    def f(self, x):\n",
        "      return np.linalg.norm(self.A.dot(x) - self.y) ** 2 / (2. ) + self.lbda * norm(x) ** 2 / 2.\n",
        "       \n",
        "    # Partial objective value\n",
        "    def f_i(self, i, x):\n",
        "       \n",
        "      return np.linalg.norm(self.A[i].dot(x) - self.y[i]) ** 2 / (2.) + self.lbda * norm(x) ** 2 / 2.\n",
        " \n",
        "    \n",
        "    # Full gradient computation\n",
        "    def grad(self, x):\n",
        "        \n",
        "    \n",
        "        return self.A.T.dot(self.A.dot(x) - self.y)  + self.lbda * x\n",
        "       \n",
        "    \n",
        "    # Partial gradient\n",
        "    def grad_i(self,i,x):\n",
        "        a_i = self.A[i]\n",
        "        \n",
        "        \n",
        "        return (a_i.dot(x) - self.y[i]) * a_i + self.lbda*x\n",
        "      \n",
        "    def fl1(self,x):\n",
        "        return self.f(x)+norm(x,ord=1)\n",
        "\n",
        "    # maximum learning rate  for the gradient\n",
        "    def tau(self):\n",
        "        C = A.transpose().dot(A)\n",
        "        L = norm(C+self.lbda*np.eye(self.p), ord=2) ** 2  \n",
        "       \n",
        "        return 1/L\n",
        "    # optimal learning rate\n",
        "    def tau_opt(self):\n",
        "      C = A.transpose().dot(A)\n",
        "      tau= 2/( norm(C+self.lbda*np.eye(self.p), ord=2) ** 2+ norm(C+self.lbda*np.eye(self.p), ord=-2) ** 2)\n",
        "      return tau"
      ],
      "metadata": {
        "id": "nwMgtErejKL2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradientdescent(prob,x0,x_min,iter,tau): ## \n",
        "  \n",
        "  s=x0\n",
        "  L=[]\n",
        "  val=0\n",
        "  for i in range(iter):\n",
        "    \n",
        "    \n",
        "    g=prob.grad(s)\n",
        "    \n",
        "    s=s - tau*g\n",
        "    \n",
        "    val=prob.f(s)\n",
        "    L.append(val)\n",
        "  return np.array(L),s\n"
      ],
      "metadata": {
        "id": "YIq59FYCjKQx"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p=8"
      ],
      "metadata": {
        "id": "i5CTGIZpjKS-"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "def coordinategradientdescent(prob,x0,xtarget,tau_opt,step0=1, n_iter=1000,nb=1,with_replace=False,GD=False): \n",
        "   \n",
        "    # Initial step: Compute and plot some initial quantities\n",
        "\n",
        "    # objective history\n",
        "    objvals = []\n",
        "    \n",
        "    # iterates distance to the minimum history\n",
        "    \n",
        "    with_replace=False\n",
        "    # Lipschitz constant\n",
        "    L = norm(A,ord=2)**2\n",
        "    \n",
        "    # Number of samples\n",
        "    n = 500\n",
        "    \n",
        "    # Initial value of current iterate  \n",
        "    x = x0.copy()\n",
        "   \n",
        "\n",
        "    # Initialize iteration counter\n",
        "    k=0\n",
        "    \n",
        "    # Current objective\n",
        "    obj = prob.f(x)\n",
        "    objvals.append(obj);\n",
        "    # Current distance to the optimum\n",
        "    \n",
        "    \n",
        "    \n",
        "  \n",
        "    # Main loop\n",
        "    while (k < n_iter):\n",
        "        # Draw the batch indices\n",
        "        # Batch gradient\n",
        "        ik=random.sample(range(0,p ), nb)\n",
        "        \n",
        "        # Stochastic gradient calculation\n",
        "        \n",
        "        sg = np.zeros(p)\n",
        "\n",
        "        for j in range(nb):\n",
        "            sg[ik[j]] = prob.grad(x)[ik[j]]\n",
        "            \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        x = x - tau_opt * sg\n",
        "            \n",
        "        \n",
        "      \n",
        "        \n",
        "        obj =prob.f(x)\n",
        "       \n",
        "        \n",
        "        k += 1\n",
        "        \n",
        "        if k*nb % 8==0:\n",
        "            \n",
        "            objvals.append(obj)\n",
        "        \n",
        "              \n",
        "    \n",
        "    # Outputs\n",
        "    x_output = x\n",
        "    \n",
        "    return x_output, np.array(objvals), \n"
      ],
      "metadata": {
        "id": "D2goYTO-jKVX"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs=60\n",
        "x0=np.zeros(8)\n",
        "prob=RegPb(A,y)\n",
        "tau_opt=prob.tau_opt()\n",
        "L5=gradientdescent(prob,x0,0,n_epochs,tau_opt)[0]\n",
        "R1=coordinategradientdescent(prob,x0,0,tau_opt/2,0, n_epochs*500,7)[1]\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.semilogy(R1, label=\"Coordinate Block Descent\", lw=2)\n",
        "\n",
        "plt.semilogy(L5, label=\"Gradient Descent\", lw=2)\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SLgCKvsujY4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "# Stochastic gradient implementation\n",
        "def coordinatestochastic(prob,x0,xtarget,stepchoice=0,step0=1, n_iter=1000,nb=1,nb1=1,with_replace=False,GD=False): \n",
        "   \n",
        "    # Initial step: Compute and plot some initial quantities\n",
        "\n",
        "    # objective history\n",
        "    objvals = []\n",
        "    \n",
        "    # iterates distance to the minimum history\n",
        "    \n",
        "    with_replace=False\n",
        "    # Lipschitz constant\n",
        "    L = norm(A,ord=2)**2\n",
        "    \n",
        "    # Number of samples\n",
        "    n = 500\n",
        "    \n",
        "    # Initial value of current iterate  \n",
        "    x = x0.copy()\n",
        "   \n",
        "\n",
        "    # Initialize iteration counter\n",
        "    k=0\n",
        "    \n",
        "    # Current objective\n",
        "    obj = prob.f(x)\n",
        "    objvals.append(obj);\n",
        "    # Current distance to the optimum\n",
        "    \n",
        "    \n",
        "    \n",
        "  \n",
        "    # Main loop\n",
        "    while (k < n_iter):\n",
        "        # Draw the batch indices\n",
        "        ik = np.random.choice(500,nb,replace=with_replace)# Batch gradient\n",
        "        jk=random.sample(range(0,p ), nb1)\n",
        "        # Stochastic gradient calculation\n",
        "        \n",
        "        sg = np.zeros(p)\n",
        "        for j in range(nb):\n",
        "            gi = prob.grad_i(ik[j],x)\n",
        "            sg = sg + gi\n",
        "        sg = (1/nb)*sg\n",
        "        tg=np.zeros(p)\n",
        "        for j in range(nb1):\n",
        "          tg[jk[j]]=sg[jk[j]]\n",
        "          \n",
        "        \n",
        "        if stepchoice==0:\n",
        "            x = x - (step0/L) * tg\n",
        "        elif stepchoice>0:\n",
        "            sk = float(step0/((k+1)**0.5))\n",
        "            \n",
        "            x = x - sk * tg\n",
        "        \n",
        "      \n",
        "        \n",
        "        obj =prob.f(x)\n",
        "       \n",
        "        \n",
        "        k += 1\n",
        "        # Plot quantities of interest at the end of every epoch only\n",
        "        if (k*nb*nb1) % 8*n == 0:\n",
        "            objvals.append(obj)\n",
        "            \n",
        "    \n",
        "    # Plot quantities of interest for the last iterate (if needed)\n",
        "    if (k*nb) % n > 0:\n",
        "        objvals.append(obj)\n",
        "        \n",
        "              \n",
        "    \n",
        "    # Outputs\n",
        "    x_output = x\n",
        "    \n",
        "    return x_output, np.array(objvals), \n"
      ],
      "metadata": {
        "id": "FTpchVBImqIu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}